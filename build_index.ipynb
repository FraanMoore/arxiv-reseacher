{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "papers = fetch_arxiv_papers(\"Language Models\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray',\n",
       " 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach',\n",
       " 'NoLiMa: Long-Context Evaluation Beyond Literal Matching',\n",
       " 'Multitwine: Multi-Object Compositing with Text and Layout Control',\n",
       " 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails',\n",
       " 'A Lightweight Method to Disrupt Memorized Sequences in LLM',\n",
       " 'Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation',\n",
       " 'CodeSCM: Causal Analysis for Multi-Modal Code Generation',\n",
       " \"An Annotated Reading of 'The Singer of Tales' in the LLM Era\",\n",
       " 'Refining Integration-by-Parts Reduction of Feynman Integrals with Machine Learning']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper[\"title\"] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"Summary: {paper['summary']}\\n\"\n",
    "            f\"Published: {paper['published']}\\n\"\n",
    "            f\"Journal Reference: {paper.get('journal_ref', 'N/A')}\\n\"\n",
    "            f\"DOI: {paper.get('doi', 'N/A')}\\n\"\n",
    "            f\"Primary Category: {paper['primary_category']}\\n\"\n",
    "            f\"Categories: {', '.join(paper['categories'])}\\n\"\n",
    "            f\"PDF URL: {paper['pdf_url']}\\n\"\n",
    "            f\"arxiv URLs: {paper['arxiv_url']}\\n\"\n",
    "        )\n",
    "\n",
    "        documents.append(Document(text=content))\n",
    "    \n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='f7f708bb-9aa6-4c81-b7d2-7e1464e8d279', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray\\nAuthors: Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, Xing Sun\\nSummary: Establishing the long-context capability of large vision-language models is\\ncrucial for video understanding, high-resolution image understanding,\\nmulti-modal agents and reasoning. We introduce Long-VITA, a simple yet\\neffective large multi-modal model for long-context visual-language\\nunderstanding tasks. It is adept at concurrently processing and analyzing\\nmodalities of image, video, and text over 4K frames or 1M tokens while\\ndelivering advanced performances on short-context multi-modal tasks. We propose\\nan effective multi-modal training schema that starts with large language models\\nand proceeds through vision-language alignment, general knowledge learning, and\\ntwo sequential stages of long-sequence fine-tuning. We further implement\\ncontext-parallelism distributed inference and logits-masked language modeling\\nhead to scale Long-VITA to infinitely long inputs of images and texts during\\nmodel inference. Regarding training data, Long-VITA is built on a mix of $17$M\\nsamples from public datasets only and demonstrates the state-of-the-art\\nperformance on various multi-modal benchmarks, compared against recent\\ncutting-edge models with internal data. Long-VITA is fully reproducible and\\nsupports both NPU and GPU platforms for training and testing. We hope Long-VITA\\ncan serve as a competitive baseline and offer valuable insights for the\\nopen-source community in advancing long-context multi-modal understanding.\\nPublished: 2025-02-07 18:59:56+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2502.05177v1\\narxiv URLs: http://arxiv.org/abs/2502.05177v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='05c28131-646c-476f-9e5a-1bf452b5d134', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach\\nAuthors: Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein\\nSummary: We study a novel language model architecture that is capable of scaling\\ntest-time computation by implicitly reasoning in latent space. Our model works\\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\\ntest-time. This stands in contrast to mainstream reasoning models that scale up\\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\\nour approach does not require any specialized training data, can work with\\nsmall context windows, and can capture types of reasoning that are not easily\\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\\nparameters and 800 billion tokens. We show that the resulting model can improve\\nits performance on reasoning benchmarks, sometimes dramatically, up to a\\ncomputation load equivalent to 50 billion parameters.\\nPublished: 2025-02-07 18:55:02+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.CL\\nPDF URL: http://arxiv.org/pdf/2502.05171v1\\narxiv URLs: http://arxiv.org/abs/2502.05171v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db435415-da1d-4b63-b883-e03445d21438', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: NoLiMa: Long-Context Evaluation Beyond Literal Matching\\nAuthors: Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Schütze\\nSummary: Recent large language models (LLMs) support long contexts ranging from 128K\\nto 1M tokens. A popular method for evaluating these capabilities is the\\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\\nof this approach include increasing distractors, fact chaining, and in-context\\nreasoning. However, in these benchmarks, models can exploit existing literal\\nmatches between the needle and haystack to simplify the task. To address this,\\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\\nneedle set, where questions and needles have minimal lexical overlap, requiring\\nmodels to infer latent associations to locate the needle within the haystack.\\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\\ntokens. While they perform well in short contexts (<1K), performance degrades\\nsignificantly as context length increases. At 32K, for instance, 10 models drop\\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\\ntop-performing exceptions, experiences a reduction from an almost-perfect\\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\\nincreased difficulty the attention mechanism faces in longer contexts when\\nliteral matches are absent, making it harder to retrieve relevant information.\\nPublished: 2025-02-07 18:49:46+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2502.05167v1\\narxiv URLs: http://arxiv.org/abs/2502.05167v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c104e7e7-7826-4aee-bdb5-0d8d61f98857', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Multitwine: Multi-Object Compositing with Text and Layout Control\\nAuthors: Gemma Canet Tarrés, Zhe Lin, Zhifei Zhang, He Zhang, Andrew Gilbert, John Collomosse, Soo Ye Kim\\nSummary: We introduce the first generative model capable of simultaneous multi-object\\ncompositing, guided by both text and layout. Our model allows for the addition\\nof multiple objects within a scene, capturing a range of interactions from\\nsimple positional relations (e.g., next to, in front of) to complex actions\\nrequiring reposing (e.g., hugging, playing guitar). When an interaction implies\\nadditional props, like `taking a selfie', our model autonomously generates\\nthese supporting objects. By jointly training for compositing and\\nsubject-driven generation, also known as customization, we achieve a more\\nbalanced integration of textual and visual inputs for text-driven object\\ncompositing. As a result, we obtain a versatile model with state-of-the-art\\nperformance in both tasks. We further present a data generation pipeline\\nleveraging visual and language models to effortlessly synthesize multimodal,\\naligned training data.\\nPublished: 2025-02-07 18:48:54+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2502.05165v1\\narxiv URLs: http://arxiv.org/abs/2502.05165v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f98261d2-8942-495c-a1c6-c4c156280977', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails\\nAuthors: Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li\\nSummary: The rapid advancement of large language models (LLMs) has increased the need\\nfor guardrail models to ensure responsible use, particularly in detecting\\nunsafe and illegal content. While substantial safety data exist in English,\\nmultilingual guardrail modeling remains underexplored due to the scarcity of\\nopen-source safety data in other languages. To address this gap, we propose a\\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\\nguardrail model co-evolve adversarially to produce high-quality synthetic data\\nfor multilingual guardrail training. We theoretically formalize this\\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\\nEmpirical evaluations show that our model \\\\ours outperforms state-of-the-art\\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\\nbenchmarks while being 4.5x faster at inference with a significantly smaller\\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\\nparticularly in addressing the imbalance for lower-resource languages in a\\ncollected real dataset. Ablation studies emphasize the critical role of\\nsynthetic data generation in bridging the imbalance in open-source data between\\nEnglish and other languages. These findings establish a scalable and efficient\\napproach to synthetic data generation, paving the way for improved multilingual\\nguardrail models to enhance LLM safety. Code, model, and data will be\\nopen-sourced at https://github.com/yihedeng9/DuoGuard.\\nPublished: 2025-02-07 18:45:03+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.LG\\nPDF URL: http://arxiv.org/pdf/2502.05163v1\\narxiv URLs: http://arxiv.org/abs/2502.05163v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12982c34-2730-4d5b-99f3-8e8717f51737', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: A Lightweight Method to Disrupt Memorized Sequences in LLM\\nAuthors: Parjanya Prajakta Prashant, Kaustubh Ponkshe, Babak Salimi\\nSummary: Large language models (LLMs) demonstrate impressive capabilities across many\\ntasks yet risk reproducing copyrighted content verbatim, raising legal and\\nethical concerns. Although methods like differential privacy or neuron editing\\ncan reduce memorization, they typically require costly retraining or direct\\naccess to model weights and may degrade performance. To address these\\nchallenges, we propose TokenSwap, a lightweight, post-hoc approach that\\nreplaces the probabilities of grammar-related tokens with those from a small\\nauxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial\\ngrade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method\\neffectively reduces well-known cases of memorized generation by upto 10x with\\nlittle to no impact on downstream tasks. Our approach offers a uniquely\\naccessible and effective solution to users of real-world systems.\\nPublished: 2025-02-07 18:41:21+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, cs.CL\\nPDF URL: http://arxiv.org/pdf/2502.05159v1\\narxiv URLs: http://arxiv.org/abs/2502.05159v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8e19d857-ae66-4dd4-b1ef-636ab9d6d4b3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation\\nAuthors: Steffen Eger, Yong Cao, Jennifer D\\'Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, Chenghua Lin, Nafise Sadat Moosavi, Wei Zhao, Tristan Miller\\nSummary: With the advent of large multimodal language models, science is now at a\\nthreshold of an AI-based technological transformation. Recently, a plethora of\\nnew AI models and tools has been proposed, promising to empower researchers and\\nacademics worldwide to conduct their research more effectively and efficiently.\\nThis includes all aspects of the research cycle, especially (1) searching for\\nrelevant literature; (2) generating research ideas and conducting\\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\\nthis survey, we provide an in-depth overview over these exciting recent\\ndevelopments, which promise to fundamentally alter the scientific research\\nprocess for good. Our survey covers the five aspects outlined above, indicating\\nrelevant datasets, methods and results (including evaluation) as well as\\nlimitations and scope for future research. Ethical concerns regarding\\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\\nharms to research integrity) take a particularly prominent place in our\\ndiscussion. We hope that our survey will not only become a reference guide for\\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\\narea of \"AI4Science\".\\nPublished: 2025-02-07 18:26:45+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI, cs.CV, cs.LG\\nPDF URL: http://arxiv.org/pdf/2502.05151v1\\narxiv URLs: http://arxiv.org/abs/2502.05151v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='390ba77b-3953-4214-88c3-c73636504606', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: CodeSCM: Causal Analysis for Multi-Modal Code Generation\\nAuthors: Mukur Gupta, Noopur Bhatt, Suman Jana\\nSummary: In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\\nanalyzing multi-modal code generation using large language models (LLMs). By\\napplying interventions to CodeSCM, we measure the causal effects of different\\nprompt modalities, such as natural language, code, and input-output examples,\\non the model. CodeSCM introduces latent mediator variables to separate the code\\nand natural language semantics of a multi-modal code generation prompt. Using\\nthe principles of Causal Mediation Analysis on these mediators we quantify\\ndirect effects representing the model's spurious leanings. We find that, in\\naddition to natural language instructions, input-output examples significantly\\ninfluence code generation.\\nPublished: 2025-02-07 18:26:15+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2502.05150v1\\narxiv URLs: http://arxiv.org/abs/2502.05150v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a381c29-69ee-4b6d-a62c-dc422c17baf0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: An Annotated Reading of 'The Singer of Tales' in the LLM Era\\nAuthors: Kush R. Varshney\\nSummary: The Parry-Lord oral-formulaic theory was a breakthrough in understanding how\\noral narrative poetry is learned, composed, and transmitted by illiterate\\nbards. In this paper, we provide an annotated reading of the mechanism\\nunderlying this theory from the lens of large language models (LLMs) and\\ngenerative artificial intelligence (AI). We point out the the similarities and\\ndifferences between oral composition and LLM generation, and comment on the\\nimplications to society and AI policy.\\nPublished: 2025-02-07 18:26:01+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CY\\nCategories: cs.CY, cs.CL\\nPDF URL: http://arxiv.org/pdf/2502.05148v1\\narxiv URLs: http://arxiv.org/abs/2502.05148v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f0bc0d4-55ee-464a-b8f8-a121f742c31f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Refining Integration-by-Parts Reduction of Feynman Integrals with Machine Learning\\nAuthors: Matt von Hippel, Matthias Wilhelm\\nSummary: Integration-by-parts reductions of Feynman integrals pose a frequent\\nbottle-neck in state-of-the-art calculations in theoretical particle and\\ngravitational-wave physics, and rely on heuristic approaches for selecting\\nintegration-by-parts identities, whose quality heavily influences the\\nperformance. In this paper, we investigate the use of machine-learning\\ntechniques to find improved heuristics. We use funsearch, a genetic programming\\nvariant based on code generation by a Large Language Model, in order to explore\\npossible approaches, then use strongly typed genetic programming to zero in on\\nuseful solutions. Both approaches manage to re-discover the state-of-the-art\\nheuristics recently incorporated into integration-by-parts solvers, and in one\\nexample find a small advance on this state of the art.\\nPublished: 2025-02-07 17:48:42+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: hep-th\\nCategories: hep-th, cs.LG, hep-ph\\nPDF URL: http://arxiv.org/pdf/2502.05121v1\\narxiv URLs: http://arxiv.org/abs/2502.05121v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from constants import embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"index/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
